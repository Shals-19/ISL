# ISL Real-Time Translation

**Real-time Indian Sign Language to English text translation model optimized for mobile deployment.**

![Architecture](docs/architecture.png)

## ğŸ“‹ Overview

This project implements a complete pipeline for translating Indian Sign Language (ISL) videos to English text. The model is specifically designed for:
- **Real-time inference** on mobile devices (Samsung Galaxy Tab S9)
- **Dense Vision Transformer** architecture for overfitting on sign language datasets
- **6000+ video training** support with capacity for fine-grained sign recognition

## ğŸ—ï¸ Architecture

```
Video Frames (224Ã—224Ã—3, 16 frames)
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dense Vision Transformer (ViT)    â”‚ â† Rich spatial feature extraction
â”‚   24 layers, 1024 hidden dim        â”‚ â† ~86M parameters
â”‚   16 attention heads                â”‚ â† Captures fine-grained details
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Temporal Convolutions (DENSE)     â”‚ â† Motion modeling (4 blocks)
â”‚   (depthwise separable + dense FFN) â”‚ â† Captures temporal dynamics
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dense Attention Pooling           â”‚ â† Compress to 32 tokens
â”‚   (learnable queries + refinement)  â”‚ â† Rich token representation
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Transformer Decoder               â”‚ â† Text generation
â”‚   (4 layers, Pre-LN)                â”‚ â† 8M parameters
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
      Text Output
```

### Architecture Specifications
- **Backbone**: Dense Vision Transformer (ViT) - Not MobileNet!
  - 24 transformer blocks (vs 12 in standard ViT)
  - 1024 hidden dimension (vs 768 in ViT-Base)
  - 16 attention heads with 4096 FFN dimension
  - Designed for overfitting on 6000-video datasets
- **Temporal Modeling**: 4 dense temporal convolution blocks
- **Attention Pooling**: Dense with refinement layers
- **Total Parameters**: ~100M (enables fine-grained sign language modeling)
- **Optimization**: Designed for overfitting on smaller sign language datasets

## ğŸš€ Quick Start

### 1. Installation

```powershell
# Navigate to project directory
cd isl_realtime

# Create virtual environment (optional)
python -m venv venv
.\venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure Paths

Edit `configs/config.yaml`:

```yaml
data:
  video_dir: "D:/datasets/iSign-videos"     # Your video directory
  csv_path: "D:/datasets/iSign_v1.1.csv"    # Your CSV path
```

### 3. Train

```powershell
# Train from scratch
python scripts/train.py --config configs/config.yaml

# Resume training
python scripts/train.py --config configs/config.yaml --resume checkpoints/best_model.pt
```

### 4. Evaluate

```powershell
python scripts/evaluate.py --checkpoint checkpoints/best_model.pt
```

### 5. Demo

```powershell
# Interactive mode
python scripts/demo.py --checkpoint checkpoints/best_model.pt --mode interactive

# Live camera
python scripts/demo.py --checkpoint checkpoints/best_model.pt --mode live

# Single video
python scripts/demo.py --checkpoint checkpoints/best_model.pt --mode video --input path/to/video.mp4
```

### 6. Export for Mobile

```powershell
python scripts/export.py --checkpoint checkpoints/best_model.pt --format onnx --quantize
```

## ğŸ“ Project Structure

```
isl_realtime/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml           # All hyperparameters
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ encoder.py        # Dense Vision Transformer + Temporal Attention
â”‚   â”‚   â”œâ”€â”€ decoder.py        # Transformer Decoder
â”‚   â”‚   â””â”€â”€ translator.py     # Full model
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset.py        # Video dataset
â”‚   â”‚   â””â”€â”€ augmentations.py  # Video augmentations
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ trainer.py        # Training loop
â”‚   â”‚   â””â”€â”€ losses.py         # CTC + CE loss
â”‚   â””â”€â”€ inference/
â”‚       â”œâ”€â”€ live.py           # Real-time inference
â”‚       â””â”€â”€ export.py         # ONNX/TFLite export
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py              # Training script
â”‚   â”œâ”€â”€ evaluate.py           # Evaluation script
â”‚   â”œâ”€â”€ demo.py               # Demo script
â”‚   â”œâ”€â”€ export.py             # Export script
â”‚   â””â”€â”€ tune.py               # Hyperparameter tuning
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ“Š Expected Results

| Epoch | Val Loss | Expected BLEU |
|-------|----------|---------------|
| 10    | ~3.0     | 5-10          |
| 30    | ~2.0     | 15-25         |
| 50    | ~1.5     | 25-35         |

**Note:** Sign Language Translation is challenging. BLEU scores of 25-35 are considered good for this task.

## ğŸ”§ Key Features

### Training
- âœ… Mixed precision training (AMP)
- âœ… Gradient accumulation
- âœ… Backbone freezing/unfreezing
- âœ… Early stopping
- âœ… Automatic checkpointing
- âœ… Separate learning rates for encoder/decoder

### Model
- âœ… **Dense Vision Transformer** backbone (1024-dim, 24 layers, 16 heads)
- âœ… Optimized for overfitting on 6000-video datasets
- âœ… 4 deep temporal convolution blocks (motion modeling)
- âœ… Dense attention pooling with refinement layers
- âœ… Pre-LayerNorm Transformer decoder (stable training)
- âœ… CTC + CE hybrid loss

### Inference
- âœ… Greedy decoding
- âœ… Beam search
- âœ… Temperature sampling
- âœ… CTC streaming mode

### Export
- âœ… ONNX export
- âœ… INT8 quantization
- âœ… TorchScript
- ğŸ”„ TFLite (via ONNX)

## ğŸ’¡ Key Improvements from Previous Attempts

| Issue | Previous | Fixed |
|-------|----------|-------|
| Encoder output length | 1568 tokens | 32 tokens (attention pooling) |
| BOS/EOS mismatch | Inconsistent IDs | Always use 101/102 (BERT) |
| Metrics | Teacher-forcing BLEU | Autoregressive BLEU |
| Model size | ~110M params | ~16M params |
| Temporal modeling | None | Temporal convolutions |
| Training stability | Unstable | Pre-LN Transformer |

## ğŸ“± Mobile Deployment

After training, export the model:

```powershell
# Export with INT8 quantization
python scripts/export.py --checkpoint checkpoints/best_model.pt --format onnx --quantize
```

The exported model can be used with:
- **Android**: ONNX Runtime for Android
- **iOS**: Core ML (convert from ONNX)

Expected mobile performance:
- **Model size**: ~20MB (INT8)
- **Inference time**: ~100-200ms per video segment

## ğŸ”¬ Hyperparameter Tuning

Run random search to find optimal hyperparameters:

```powershell
python scripts/tune.py --config configs/config.yaml --trials 20 --epochs 5
```

## âš ï¸ Common Issues

### CUDA Out of Memory
Reduce batch size in `config.yaml`:
```yaml
training:
  batch_size: 16  # Reduce from 32
```

### Slow Training
- Enable mixed precision: `use_amp: true`
- Reduce `num_workers` if I/O bound
- Use SSD for video storage

### Poor Results
1. Check data paths in config
2. Ensure videos have consistent quality
3. Increase training epochs
4. Try hyperparameter tuning

## ğŸ“„ License

This project is for educational purposes (5th semester project).

## ğŸ™ Acknowledgments

- iSign dataset from HuggingFace
- Vision Transformer from timm (PyTorch Image Models)
- BERT tokenizer from HuggingFace Transformers
- Architecture optimized for sign language recognition
